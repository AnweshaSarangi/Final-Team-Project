{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd64c685",
   "metadata": {},
   "source": [
    "## Model Selection for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf968514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Check if required\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81531aac",
   "metadata": {},
   "source": [
    "### Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98c2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the newly uploaded CSV file\n",
    "file_path = 'C:/Users/Soumiz/Anaconda/USD/full_reduced_data.csv'\n",
    "full_reduced_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172130aa",
   "metadata": {},
   "source": [
    "#### Prepare Data for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bfd63",
   "metadata": {},
   "source": [
    "<Font size=5px; color='blue'> Dataset is imbalnced <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffcbd39",
   "metadata": {},
   "source": [
    "##### Addressing Imbalance dataset Issue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638290c",
   "metadata": {},
   "source": [
    "<Font size=3px; color='darkblue'> We decided to downsample other classes <br>\n",
    "    Target class has minimum occurance <br>\n",
    "    Hence, other classes are divided into multiple sets of same size <br>\n",
    "    Then one file from each class are combined to get a balanced dataset for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a8593",
   "metadata": {},
   "source": [
    "##### Downsampling  the other two classes to create balance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23aae7b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in dbts_0: 167650\n",
      "Number of rows in dbts_1: 3729\n",
      "Number of rows in dbts_2: 26215\n",
      "Smallest length: 3729\n",
      "Data partitioning complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into three separate files based on Diabetes_012 value\n",
    "dbts_0 = full_reduced_data[full_reduced_data['Diabetes_012'] == 0]\n",
    "dbts_1 = full_reduced_data[full_reduced_data['Diabetes_012'] == 1]\n",
    "dbts_2 = full_reduced_data[full_reduced_data['Diabetes_012'] == 2]\n",
    "\n",
    "# Save these files\n",
    "dbts_0.to_csv('dbts_0.csv', index=False)\n",
    "dbts_1.to_csv('dbts_1.csv', index=False)\n",
    "dbts_2.to_csv('dbts_2.csv', index=False)\n",
    "\n",
    "# Count the number of rows for each data file\n",
    "count_0 = len(dbts_0)\n",
    "count_1 = len(dbts_1)\n",
    "count_2 = len(dbts_2)\n",
    "\n",
    "print(f\"Number of rows in dbts_0: {count_0}\")\n",
    "print(f\"Number of rows in dbts_1: {count_1}\")\n",
    "print(f\"Number of rows in dbts_2: {count_2}\")\n",
    "\n",
    "# Find the smallest file length\n",
    "smallest_length = min(count_0, count_1, count_2)\n",
    "\n",
    "print(f\"Smallest length: {smallest_length}\")\n",
    "\n",
    "# Function to partition a dataframe into smaller parts\n",
    "def partition_dataframe(df, size, file_prefix):\n",
    "    num_parts = len(df) // size\n",
    "    for i in range(num_parts + 1):\n",
    "        part = df[i*size:(i+1)*size]\n",
    "        if not part.empty:\n",
    "            part.to_csv(f'{file_prefix}_p{i+1}.csv', index=False)\n",
    "\n",
    "# Partition the dataframes\n",
    "partition_dataframe(dbts_0, smallest_length, 'dbts_0')\n",
    "partition_dataframe(dbts_1, smallest_length, 'dbts_1')\n",
    "partition_dataframe(dbts_2, smallest_length, 'dbts_2')\n",
    "\n",
    "print(\"Data partitioning complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b903e0",
   "metadata": {},
   "source": [
    "#### Combining files from class 0, 1 and 2 using all possible combination to get large amount of datasets \n",
    "\n",
    "<Font size=3px; color='darkgreen'> Last file of both class 0 and class 2 are skipped as they are not having same dimention as of class 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef63461",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Number of files of different classes:\n",
    "x_values = range(1,45)  # Adjust the range as needed\n",
    "y_values = range(1,8)  # Adjust the range as needed\n",
    "\n",
    "def load_and_combine_files(x, y):\n",
    "    # Define file paths\n",
    "    file1 = f'dbts_0_p{x}.csv'\n",
    "    file2 = 'dbts_1_p1.csv'\n",
    "    file3 = f'dbts_2_p{y}.csv'\n",
    "\n",
    "    # Load the data\n",
    "    data1 = pd.read_csv(file1)\n",
    "    data2 = pd.read_csv(file2)\n",
    "    data3 = pd.read_csv(file3)\n",
    "\n",
    "    # Concatenate the data\n",
    "    combined_data = pd.concat([data1, data2, data3], ignore_index=True)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "\n",
    "count=0\n",
    "for x in x_values:\n",
    "    for y in y_values:\n",
    "        combined_data = load_and_combine_files(x, y)\n",
    "        # You can now use combined_data for further analysis\n",
    "        # For example, save the combined data to a new file\n",
    "        combined_data.to_csv(f'C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_{x}_{y}.csv', index=False)\n",
    "\n",
    "        '''# Print the shape of the combined data for verification\n",
    "        print(f'Combined data for x={x} and y={y} has shape: {combined_data.shape}')'''\n",
    "        count+=1\n",
    "print(\"Total file created\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048fb82",
   "metadata": {},
   "source": [
    "<Font size=3px; color='darkblue'>\n",
    "    \n",
    "### Model Testing and Evaluation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89c0c1",
   "metadata": {},
   "source": [
    "#### Models selected for Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a7d8a",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\"> 1. Logistic Regression  </span> <br>\n",
    "Pros: Simple, interpretable, works well with linearly separable data.  <br>\n",
    "Cons: Might not capture complex relationships in the data.  <br>\n",
    "Usage: Good starting point, especially for binary classification and when interpretability is important.  <br>\n",
    "    \n",
    "<span style=\"color: blue;\">2. Random Forest  </span> <br>\n",
    "Pros: Handles high-dimensional data well, robust to overfitting, interpretable (feature importance).  <br>\n",
    "Cons: Computationally intensive for large datasets.  <br>\n",
    "Usage: Good for datasets with many features, captures complex interactions, provides feature importance.  <br>\n",
    "\n",
    "<span style=\"color: blue;\">3. Gradient Boosting (e.g., XGBoost, LightGBM)  </span> <br>\n",
    "Pros: High accuracy, handles missing values, feature importance.  <br>\n",
    "Cons: Requires careful tuning, longer training times.  <br>\n",
    "Usage: Good for improving performance over simpler models, handles complex data well.  <br>\n",
    "    \n",
    "<span style=\"color: blue;\">4. Support Vector Machine (SVM)  </span> <br>\n",
    "Pros: Effective in high-dimensional spaces, robust to overfitting (especially with the kernel trick).  <br>\n",
    "Cons: Computationally intensive for large datasets, less interpretable.  <br>\n",
    "Usage: Effective for smaller to medium-sized datasets, especially with non-linear decision boundaries.  <br>\n",
    "    \n",
    "<span style=\"color: blue;\">5. Artificial Neural Networks (ANN)  </span> <br>\n",
    "Pros: Captures complex patterns, scalable to large datasets.  <br>\n",
    "Cons: Requires large amounts of data, computationally intensive, less interpretable.  <br>\n",
    "Usage: Good for very large datasets and complex patterns, but requires tuning and computational resources.  <br>\n",
    "\n",
    "<span style=\"color: blue;\">6. K-Nearest Neighbors (KNN)  </span> <br>\n",
    "Pros: Simple, no training phase.  <br>\n",
    "Cons: Computationally expensive during prediction, sensitive to irrelevant features.  <br>\n",
    "Usage: Good for small datasets with clear clusters, not suitable for high-dimensional data.  <br>\n",
    "\n",
    "<span style=\"color: blue;\">7. Naive Bayes  </span> <br>\n",
    "Pros: Simple, fast, works well with small datasets and text classification.  <br>\n",
    "Cons: Assumes feature independence (which might not hold), less effective for complex datasets.  <br>\n",
    "Usage: Effective for text classification, smaller datasets, and where independence assumption is reasonable.  <br>\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8b36a",
   "metadata": {},
   "source": [
    "<Font size=3px; color='darkblue'>\n",
    "\n",
    "\n",
    "#### Algorithm for single dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b2050",
   "metadata": {},
   "source": [
    "#### Create Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc31120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file\n",
    "file_path = 'C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_1.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = data.drop(columns=['Diabetes_012'])\n",
    "y = data['Diabetes_012']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f752905",
   "metadata": {},
   "source": [
    "#### Evaluate Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5753e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy  Precision    Recall  F1 Score   AUC-ROC\n",
      "Logistic Regression  0.498213   0.492685  0.498213  0.488772  0.690130\n",
      "Random Forest        0.459786   0.458018  0.459786  0.458731  0.644031\n",
      "Gradient Boosting    0.500894   0.500227  0.500894  0.497710  0.690707\n",
      "SVM                  0.484361   0.480112  0.484361  0.463826  0.674263\n",
      "KNN                  0.424039   0.418547  0.424039  0.417092  0.605976\n",
      "Naive Bayes          0.478105   0.470046  0.478105  0.458686  0.671191\n",
      "Neural Network       0.494191   0.500032  0.494191  0.495642  0.684694\n",
      "Best models by each metric:\n",
      "{'Accuracy': 'Gradient Boosting', 'Precision': 'Gradient Boosting', 'Recall': 'Gradient Boosting', 'F1 Score': 'Gradient Boosting', 'AUC-ROC': 'Gradient Boosting'}\n",
      "\n",
      "Model counts:\n",
      "Gradient Boosting    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Best model based on highest count: Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
    "    'SVM': SVC(kernel='linear', probability=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_prob = model.predict_proba(X_test)\n",
    "        auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
    "    else:\n",
    "        auc = None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    accuracy, precision, recall, f1, auc = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC-ROC': auc\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Identify the best model for each metric\n",
    "best_models_per_metric = {\n",
    "    'Accuracy': results_df['Accuracy'].idxmax(),\n",
    "    'Precision': results_df['Precision'].idxmax(),\n",
    "    'Recall': results_df['Recall'].idxmax(),\n",
    "    'F1 Score': results_df['F1 Score'].idxmax(),\n",
    "    'AUC-ROC': results_df['AUC-ROC'].idxmax() if 'AUC-ROC' in results_df.columns else None\n",
    "}\n",
    "\n",
    "# Remove AUC-ROC if all are None\n",
    "if best_models_per_metric['AUC-ROC'] is None:\n",
    "    del best_models_per_metric['AUC-ROC']\n",
    "\n",
    "# Count the number of times each model is the best across different metrics\n",
    "model_counts = pd.Series(best_models_per_metric.values()).value_counts()\n",
    "\n",
    "# Determine which metric has the highest count of best-performing models\n",
    "max_count = model_counts.max()\n",
    "best_models_by_count = model_counts[model_counts == max_count].index.tolist()\n",
    "\n",
    "# Print results\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "# Display Performance Details\n",
    "print(\"Best models by each metric:\")\n",
    "print(best_models_per_metric)\n",
    "print(\"\\nModel counts:\")\n",
    "print(model_counts)\n",
    "\n",
    "# Print Best Model\n",
    "if len(best_models_by_count) == 1:\n",
    "    print(f\"\\nBest model based on highest count: {best_models_by_count[0]}\")\n",
    "elif len(best_models_by_count) > 1:\n",
    "    print(f\"\\nMultiple models have the highest count: {', '.join(best_models_by_count)}\")\n",
    "else:\n",
    "    print(\"\\nNo significant model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89eee59",
   "metadata": {},
   "source": [
    "result of file 1_2\n",
    "|                    | Accuracy  |  Precision    |   Recall | F1 Score\n",
    "|--------------------|-----------|--------------|--------|----------|  \n",
    "| Logistic Regression | 0.490617 |  0.480787 | 0.490617 | 0.478728 |\n",
    "Random Forest     |   0.478999 |  0.476718 | 0.478999 | 0.477755 |\n",
    "Gradient Boosting  |  0.504021 |  0.500586 | 0.504021 |  0.497561 |\n",
    "SVM                |  0.496425 |  0.492247 | 0.496425 | 0.472999 |\n",
    "KNN                |  0.437891 |  0.435411 | 0.437891 | 0.431071 |\n",
    "Naive Bayes       |   0.484361 |  0.474064 | 0.484361 | 0.459248 |\n",
    "Neural Network    |   0.496872 |  0.484121 | 0.496872 | 0.469677 |\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3c460",
   "metadata": {},
   "source": [
    "result of file 3_2\n",
    "|                    | Accuracy | Precision  |  Recall | F1 Score|\n",
    "|--------------------|-----------|--------------|--------|----------| \n",
    "|Logistic Regression | 0.478105 |  0.463749 | 0.478105 | 0.463690|\n",
    "|Random Forest       | 0.443700 |  0.443413 | 0.443700 | 0.443526|\n",
    "|Gradient Boosting   | 0.475871 |  0.466678 | 0.475871 | 0.467335|\n",
    "|SVM                 | 0.478999 |  0.467690 | 0.478999 | 0.446045|\n",
    "|KNN                 | 0.423146 |  0.421522 | 0.423146 | 0.418178|\n",
    "|Naive Bayes         | 0.469616 |  0.455770 | 0.469616 | 0.445430|\n",
    "|Neural Network      | 0.485255 |  0.478683 | 0.485255 | 0.479133|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c51b8",
   "metadata": {},
   "source": [
    "<Font size=3px; color='darkblue'>\n",
    "    \n",
    "### Model Testing and Evaluation  <br>\n",
    "    This is used for model selection in this project\n",
    "\n",
    "#### 30 randomly selected datasets are used from the entire database for evaluation\n",
    "    \n",
    "The model demonstrated best performance for maximum number of datasets will be selected as the \n",
    "    <b> Best fit model</b>\n",
    "    for this classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Base directory and file pattern\n",
    "base_dir = 'C:/Users/Soumiz/Anaconda/USD/Combined Data/'\n",
    "file_pattern = 'combined_data_{i}_{j}.csv'\n",
    "\n",
    "# List to hold best models\n",
    "best_models = []\n",
    "\n",
    "# Initializing the used_index list\n",
    "used_index = []\n",
    "\n",
    "p = 46 # total number of files in class 0\n",
    "q = 8   # total number of files in class 2 \n",
    "k=30 # Total number of dataset will be used for evaluation \n",
    "\n",
    "# Repeate Operation for ten times \n",
    "for num in range(k):   \n",
    "    # Create a list of numbers from 1 to x with step height 1 for selection of dataset \n",
    "    m = list(range(1, p))\n",
    "    n= list(range(1, q))\n",
    "   \n",
    "    # Randomly select one number from each list as part of file name format\n",
    "    temp1 = random.choice(m)\n",
    "    temp2 = random.choice(n)\n",
    "   \n",
    "    if (temp1 * 10 + temp2) in used_index:\n",
    "        k-=1\n",
    "        continue\n",
    "    else:\n",
    "        # Appending the calculated value to the list\n",
    "        used_index.append(temp1 * 10 + temp2)\n",
    "\n",
    "    # Generate the file path\n",
    "    file_path = os.path.join(base_dir, file_pattern.format(i=temp1, j=temp2))\n",
    "    # Read the data file and append to the list\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        # Split data into train and test sets\n",
    "        X = data.drop(columns=['Diabetes_012'])\n",
    "        y = data['Diabetes_012']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # List of models to evaluate\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
    "            'SVM': SVC(kernel='linear', probability=True),\n",
    "            'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "            'Naive Bayes': GaussianNB(),\n",
    "            'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "        }\n",
    "\n",
    "        # Function to evaluate models\n",
    "        def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_pred_prob = model.predict_proba(X_test)\n",
    "                auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
    "            else:\n",
    "                auc = None\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            return accuracy, precision, recall, f1, auc\n",
    "\n",
    "        # Evaluate all models\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            accuracy, precision, recall, f1, auc = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "            results[name] = {\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1 Score': f1,\n",
    "                'AUC-ROC': auc\n",
    "            }\n",
    "\n",
    "        # Convert results to DataFrame for better visualization\n",
    "        results_df = pd.DataFrame(results).T\n",
    "\n",
    "        # Identify the best model for each metric\n",
    "        best_models_per_metric = {\n",
    "            'Accuracy': results_df['Accuracy'].idxmax(),\n",
    "            'Precision': results_df['Precision'].idxmax(),\n",
    "            'Recall': results_df['Recall'].idxmax(),\n",
    "            'F1 Score': results_df['F1 Score'].idxmax(),\n",
    "            'AUC-ROC': results_df['AUC-ROC'].idxmax() if 'AUC-ROC' in results_df.columns else None\n",
    "        }\n",
    "\n",
    "        # Remove AUC-ROC if all are None\n",
    "        if best_models_per_metric['AUC-ROC'] is None:\n",
    "            del best_models_per_metric['AUC-ROC']\n",
    "\n",
    "        # Count the number of times each model is the best across different metrics\n",
    "        model_counts = pd.Series(best_models_per_metric.values()).value_counts()\n",
    "\n",
    "        # Determine which metric has the highest count of best-performing models\n",
    "        max_count = model_counts.max()\n",
    "        best_models_by_count = model_counts[model_counts == max_count].index.tolist()\n",
    "        print(temp1,\"  \",temp2,\"  \",\"\\n\",best_models_by_count)\n",
    "\n",
    "        # Save Best Model name to final list of best_models\n",
    "        if len(best_models_by_count) == 1:\n",
    "            best_models.append(best_models_by_count[0])\n",
    "        elif len(best_models_by_count) > 1 & len(best_models_by_count)< len(model_counts):\n",
    "            best_models.extend(best_models_by_count)\n",
    "        else:\n",
    "            best_models.append('Null')  # Handle the case where no significant models are found\n",
    "\n",
    "        print(f\"Successfully read {file_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"File {file_path} is empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Determine which model has the highest count of best-performing metrics\n",
    "models_count=pd.Series(best_models).value_counts()\n",
    "\n",
    "# Convert list to Series and count occurrences of each model\n",
    "models_count = pd.Series(best_models).value_counts()\n",
    "\n",
    "# Find the maximum count\n",
    "max_count = models_count.max()\n",
    "\n",
    "# Filter models_count to get model names with the highest count\n",
    "models_with_max_count = models_count[models_count == max_count].index.tolist()\n",
    "\n",
    "\n",
    "# Print Best Model\n",
    "\n",
    "if len(models_with_max_count)==1:\n",
    "    print(\"The Best model based on analysis:\",models_with_max_count)\n",
    "elif len(models_with_max_count)<len(best_models):\n",
    "        print(f\"\\nMultiple models have the highest count: {', '.join(models_with_max_count)}\")\n",
    "else:\n",
    "        print(\"\\nNo significant model.\")  # Handle the case where no significant models are found\n",
    "\n",
    "\n",
    "        \n",
    "# Calculate and print the total execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d6979",
   "metadata": {},
   "source": [
    "### Models tested on all 308 created datasets which are balanced \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746cb913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_1.csv\n",
      "1    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_2.csv\n",
      "1    3    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_3.csv\n",
      "1    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_4.csv\n",
      "1    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_5.csv\n",
      "1    6    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_6.csv\n",
      "1    7    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_1_7.csv\n",
      "2    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_2_1.csv\n",
      "2    2    \n",
      " ['SVM', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_2_2.csv\n",
      "2    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_2_3.csv\n",
      "2    4    \n",
      " ['Naive Bayes']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_2_4.csv\n",
      "2    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_2_5.csv\n",
      "2    6    \n",
      " ['Naive Bayes', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_2_6.csv\n",
      "2    7    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_2_7.csv\n",
      "3    1    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_3_1.csv\n",
      "3    2    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_3_2.csv\n",
      "3    3    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_3_3.csv\n",
      "3    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_3_4.csv\n",
      "3    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_3_5.csv\n",
      "3    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_3_6.csv\n",
      "3    7    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_3_7.csv\n",
      "4    1    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_4_1.csv\n",
      "4    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_4_2.csv\n",
      "4    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_4_3.csv\n",
      "4    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_4_4.csv\n",
      "4    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_4_5.csv\n",
      "4    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_4_6.csv\n",
      "4    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_4_7.csv\n",
      "5    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_5_1.csv\n",
      "5    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_5_2.csv\n",
      "5    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_5_3.csv\n",
      "5    4    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_5_4.csv\n",
      "5    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_5_5.csv\n",
      "5    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_5_6.csv\n",
      "5    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_5_7.csv\n",
      "6    1    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_6_1.csv\n",
      "6    2    \n",
      " ['SVM']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_6_2.csv\n",
      "6    3    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_6_3.csv\n",
      "6    4    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_6_4.csv\n",
      "6    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_6_5.csv\n",
      "6    6    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_6_6.csv\n",
      "6    7    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_6_7.csv\n",
      "7    1    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_7_1.csv\n",
      "7    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_7_2.csv\n",
      "7    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_7_3.csv\n",
      "7    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_7_4.csv\n",
      "7    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_7_5.csv\n",
      "7    6    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_7_6.csv\n",
      "7    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_7_7.csv\n",
      "8    1    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_8_1.csv\n",
      "8    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_8_2.csv\n",
      "8    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_8_3.csv\n",
      "8    4    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_8_4.csv\n",
      "8    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_8_5.csv\n",
      "8    6    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_8_6.csv\n",
      "8    7    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_8_7.csv\n",
      "9    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_9_1.csv\n",
      "9    2    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_9_2.csv\n",
      "9    3    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_9_3.csv\n",
      "9    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_9_4.csv\n",
      "9    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_9_5.csv\n",
      "9    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_9_6.csv\n",
      "9    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_9_7.csv\n",
      "10    1    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_10_1.csv\n",
      "10    2    \n",
      " ['Gradient Boosting', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_10_2.csv\n",
      "10    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_10_3.csv\n",
      "10    4    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_10_4.csv\n",
      "10    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_10_5.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_10_6.csv\n",
      "10    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_10_7.csv\n",
      "11    1    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_11_1.csv\n",
      "11    2    \n",
      " ['SVM']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_11_2.csv\n",
      "11    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_11_3.csv\n",
      "11    4    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_11_4.csv\n",
      "11    5    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_11_5.csv\n",
      "11    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_11_6.csv\n",
      "11    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_11_7.csv\n",
      "12    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_12_1.csv\n",
      "12    2    \n",
      " ['SVM', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_12_2.csv\n",
      "12    3    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_12_3.csv\n",
      "12    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_12_4.csv\n",
      "12    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_12_5.csv\n",
      "12    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_12_6.csv\n",
      "12    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_12_7.csv\n",
      "13    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_13_1.csv\n",
      "13    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_13_2.csv\n",
      "13    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_13_3.csv\n",
      "13    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_13_4.csv\n",
      "13    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_13_5.csv\n",
      "13    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_13_6.csv\n",
      "13    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_13_7.csv\n",
      "14    1    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_14_1.csv\n",
      "14    2    \n",
      " ['SVM']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_14_2.csv\n",
      "14    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_14_3.csv\n",
      "14    4    \n",
      " ['SVM', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_14_4.csv\n",
      "14    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_14_5.csv\n",
      "14    6    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_14_6.csv\n",
      "14    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_14_7.csv\n",
      "15    1    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_15_1.csv\n",
      "15    2    \n",
      " ['SVM', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_15_2.csv\n",
      "15    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_15_3.csv\n",
      "15    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_15_4.csv\n",
      "15    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_15_5.csv\n",
      "15    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_15_6.csv\n",
      "15    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_15_7.csv\n",
      "16    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_16_1.csv\n",
      "16    2    \n",
      " ['SVM', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_16_2.csv\n",
      "16    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_16_3.csv\n",
      "16    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_16_4.csv\n",
      "16    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_16_5.csv\n",
      "16    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_16_6.csv\n",
      "16    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_16_7.csv\n",
      "17    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_17_1.csv\n",
      "17    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_17_2.csv\n",
      "17    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_17_3.csv\n",
      "17    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_17_4.csv\n",
      "17    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_17_5.csv\n",
      "17    6    \n",
      " ['Naive Bayes']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_17_6.csv\n",
      "17    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_17_7.csv\n",
      "18    1    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_18_1.csv\n",
      "18    2    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_18_2.csv\n",
      "18    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_18_3.csv\n",
      "18    4    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_18_4.csv\n",
      "18    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_18_5.csv\n",
      "18    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_18_6.csv\n",
      "18    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_18_7.csv\n",
      "19    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_19_1.csv\n",
      "19    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_19_2.csv\n",
      "19    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_19_3.csv\n",
      "19    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_19_4.csv\n",
      "19    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_19_5.csv\n",
      "19    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_19_6.csv\n",
      "19    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_19_7.csv\n",
      "20    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_20_1.csv\n",
      "20    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_20_2.csv\n",
      "20    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_20_3.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_20_4.csv\n",
      "20    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_20_5.csv\n",
      "20    6    \n",
      " ['Naive Bayes', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_20_6.csv\n",
      "20    7    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_20_7.csv\n",
      "21    1    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_21_1.csv\n",
      "21    2    \n",
      " ['SVM', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_21_2.csv\n",
      "21    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_21_3.csv\n",
      "21    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_21_4.csv\n",
      "21    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_21_5.csv\n",
      "21    6    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_21_6.csv\n",
      "21    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_21_7.csv\n",
      "22    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_22_1.csv\n",
      "22    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_22_2.csv\n",
      "22    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_22_3.csv\n",
      "22    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_22_4.csv\n",
      "22    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_22_5.csv\n",
      "22    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_22_6.csv\n",
      "22    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_22_7.csv\n",
      "23    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_23_1.csv\n",
      "23    2    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_23_2.csv\n",
      "23    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_23_3.csv\n",
      "23    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_23_4.csv\n",
      "23    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_23_5.csv\n",
      "23    6    \n",
      " ['Naive Bayes', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_23_6.csv\n",
      "23    7    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_23_7.csv\n",
      "24    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_24_1.csv\n",
      "24    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_24_2.csv\n",
      "24    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_24_3.csv\n",
      "24    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_24_4.csv\n",
      "24    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_24_5.csv\n",
      "24    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_24_6.csv\n",
      "24    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_24_7.csv\n",
      "25    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_25_1.csv\n",
      "25    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_25_2.csv\n",
      "25    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_25_3.csv\n",
      "25    4    \n",
      " ['Naive Bayes']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_25_4.csv\n",
      "25    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_25_5.csv\n",
      "25    6    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_25_6.csv\n",
      "25    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_25_7.csv\n",
      "26    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_26_1.csv\n",
      "26    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_26_2.csv\n",
      "26    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_26_3.csv\n",
      "26    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_26_4.csv\n",
      "26    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_26_5.csv\n",
      "26    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_26_6.csv\n",
      "26    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_26_7.csv\n",
      "27    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_27_1.csv\n",
      "27    2    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_27_2.csv\n",
      "27    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_27_3.csv\n",
      "27    4    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_27_4.csv\n",
      "27    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_27_5.csv\n",
      "27    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_27_6.csv\n",
      "27    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_27_7.csv\n",
      "28    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_28_1.csv\n",
      "28    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_28_2.csv\n",
      "28    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_28_3.csv\n",
      "28    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_28_4.csv\n",
      "28    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_28_5.csv\n",
      "28    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_28_6.csv\n",
      "28    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_28_7.csv\n",
      "29    1    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_29_1.csv\n",
      "29    2    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_29_2.csv\n",
      "29    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_29_3.csv\n",
      "29    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_29_4.csv\n",
      "29    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_29_5.csv\n",
      "29    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_29_6.csv\n",
      "29    7    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_29_7.csv\n",
      "30    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_30_1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_30_2.csv\n",
      "30    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_30_3.csv\n",
      "30    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_30_4.csv\n",
      "30    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_30_5.csv\n",
      "30    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_30_6.csv\n",
      "30    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_30_7.csv\n",
      "31    1    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_31_1.csv\n",
      "31    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_31_2.csv\n",
      "31    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_31_3.csv\n",
      "31    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_31_4.csv\n",
      "31    5    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_31_5.csv\n",
      "31    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_31_6.csv\n",
      "31    7    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_31_7.csv\n",
      "32    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_32_1.csv\n",
      "32    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_32_2.csv\n",
      "32    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_32_3.csv\n",
      "32    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_32_4.csv\n",
      "32    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_32_5.csv\n",
      "32    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_32_6.csv\n",
      "32    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_32_7.csv\n",
      "33    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_33_1.csv\n",
      "33    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_33_2.csv\n",
      "33    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_33_3.csv\n",
      "33    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_33_4.csv\n",
      "33    5    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_33_5.csv\n",
      "33    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_33_6.csv\n",
      "33    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_33_7.csv\n",
      "34    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_34_1.csv\n",
      "34    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_34_2.csv\n",
      "34    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_34_3.csv\n",
      "34    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_34_4.csv\n",
      "34    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_34_5.csv\n",
      "34    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_34_6.csv\n",
      "34    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_34_7.csv\n",
      "35    1    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_35_1.csv\n",
      "35    2    \n",
      " ['Naive Bayes', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_35_2.csv\n",
      "35    3    \n",
      " ['Naive Bayes']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_35_3.csv\n",
      "35    4    \n",
      " ['Naive Bayes', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_35_4.csv\n",
      "35    5    \n",
      " ['Naive Bayes', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_35_5.csv\n",
      "35    6    \n",
      " ['Naive Bayes', 'Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_35_6.csv\n",
      "35    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_35_7.csv\n",
      "36    1    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_36_1.csv\n",
      "36    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_36_2.csv\n",
      "36    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_36_3.csv\n",
      "36    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_36_4.csv\n",
      "36    5    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_36_5.csv\n",
      "36    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_36_6.csv\n",
      "36    7    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_36_7.csv\n",
      "37    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_37_1.csv\n",
      "37    2    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_37_2.csv\n",
      "37    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_37_3.csv\n",
      "37    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_37_4.csv\n",
      "37    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_37_5.csv\n",
      "37    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_37_6.csv\n",
      "37    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_37_7.csv\n",
      "38    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_38_1.csv\n",
      "38    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_38_2.csv\n",
      "38    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_38_3.csv\n",
      "38    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_38_4.csv\n",
      "38    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_38_5.csv\n",
      "38    6    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_38_6.csv\n",
      "38    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_38_7.csv\n",
      "39    1    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_39_1.csv\n",
      "39    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_39_2.csv\n",
      "39    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_39_3.csv\n",
      "39    4    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_39_4.csv\n",
      "39    5    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_39_5.csv\n",
      "39    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_39_6.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39    7    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_39_7.csv\n",
      "40    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_40_1.csv\n",
      "40    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_40_2.csv\n",
      "40    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_40_3.csv\n",
      "40    4    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_40_4.csv\n",
      "40    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_40_5.csv\n",
      "40    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_40_6.csv\n",
      "40    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_40_7.csv\n",
      "41    1    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_41_1.csv\n",
      "41    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_41_2.csv\n",
      "41    3    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_41_3.csv\n",
      "41    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_41_4.csv\n",
      "41    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_41_5.csv\n",
      "41    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_41_6.csv\n",
      "41    7    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_41_7.csv\n",
      "42    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_42_1.csv\n",
      "42    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_42_2.csv\n",
      "42    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_42_3.csv\n",
      "42    4    \n",
      " ['SVM']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_42_4.csv\n",
      "42    5    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_42_5.csv\n",
      "42    6    \n",
      " ['Logistic Regression', 'Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_42_6.csv\n",
      "42    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_42_7.csv\n",
      "43    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_43_1.csv\n",
      "43    2    \n",
      " ['Logistic Regression', 'Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_43_2.csv\n",
      "43    3    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_43_3.csv\n",
      "43    4    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_43_4.csv\n",
      "43    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_43_5.csv\n",
      "43    6    \n",
      " ['Logistic Regression']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_43_6.csv\n",
      "43    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_43_7.csv\n",
      "44    1    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_44_1.csv\n",
      "44    2    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_44_2.csv\n",
      "44    3    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_44_3.csv\n",
      "44    4    \n",
      " ['Naive Bayes']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_44_4.csv\n",
      "44    5    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_44_5.csv\n",
      "44    6    \n",
      " ['Neural Network']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_44_6.csv\n",
      "44    7    \n",
      " ['Gradient Boosting']\n",
      "Successfully read C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_44_7.csv\n",
      "File C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_45_1.csv not found.\n",
      "File C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_45_2.csv not found.\n",
      "File C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_45_3.csv not found.\n",
      "File C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_45_4.csv not found.\n",
      "File C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_45_5.csv not found.\n",
      "File C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_45_6.csv not found.\n",
      "File C:/Users/Soumiz/Anaconda/USD/Combined Data/combined_data_45_7.csv not found.\n",
      "There is only 1 dataset and Best model based on analysis: Gradient Boosting\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'end_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 149\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNo significant model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Handle the case where no significant models are found\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Calculate and print the total execution time\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal execution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecution_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'end_time' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Base directory and file pattern\n",
    "base_dir = 'C:/Users/Soumiz/Anaconda/USD/Combined Data/'\n",
    "file_pattern = 'combined_data_{i}_{j}.csv'\n",
    "\n",
    "# List to hold best models\n",
    "best_models = []\n",
    "\n",
    "# Loop through the range of i and j\n",
    "for i in range(1, 46):   # replace by range(1,46) for full range\n",
    "    for j in range(1, 8):   # replace by range(1,8) for full range\n",
    "        # Generate the file path\n",
    "        file_path = os.path.join(base_dir, file_pattern.format(i=i, j=j))\n",
    "        # Read the data file and append to the list\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            # Split data into train and test sets\n",
    "            X = data.drop(columns=['Diabetes_012'])\n",
    "            y = data['Diabetes_012']\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # List of models to evaluate\n",
    "            models = {\n",
    "                'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "                'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "                'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
    "                'SVM': SVC(kernel='linear', probability=True),\n",
    "                'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "                'Naive Bayes': GaussianNB(),\n",
    "                'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "            }\n",
    "\n",
    "            # Function to evaluate models\n",
    "            def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    y_pred_prob = model.predict_proba(X_test)\n",
    "                    auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
    "                else:\n",
    "                    auc = None\n",
    "\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "                return accuracy, precision, recall, f1, auc\n",
    "\n",
    "            # Evaluate all models\n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                accuracy, precision, recall, f1, auc = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "                results[name] = {\n",
    "                    'Accuracy': accuracy,\n",
    "                    'Precision': precision,\n",
    "                    'Recall': recall,\n",
    "                    'F1 Score': f1,\n",
    "                    'AUC-ROC': auc\n",
    "                }\n",
    "\n",
    "            # Convert results to DataFrame for better visualization\n",
    "            results_df = pd.DataFrame(results).T\n",
    "\n",
    "            # Identify the best model for each metric\n",
    "            best_models_per_metric = {\n",
    "                'Accuracy': results_df['Accuracy'].idxmax(),\n",
    "                'Precision': results_df['Precision'].idxmax(),\n",
    "                'Recall': results_df['Recall'].idxmax(),\n",
    "                'F1 Score': results_df['F1 Score'].idxmax(),\n",
    "                'AUC-ROC': results_df['AUC-ROC'].idxmax() if 'AUC-ROC' in results_df.columns else None\n",
    "            }\n",
    "\n",
    "            # Remove AUC-ROC if all are None\n",
    "            if best_models_per_metric['AUC-ROC'] is None:\n",
    "                del best_models_per_metric['AUC-ROC']\n",
    "\n",
    "            # Count the number of times each model is the best across different metrics\n",
    "            model_counts = pd.Series(best_models_per_metric.values()).value_counts()\n",
    "\n",
    "            # Determine which metric has the highest count of best-performing models\n",
    "            max_count = model_counts.max()\n",
    "            best_models_by_count = model_counts[model_counts == max_count].index.tolist()\n",
    "            print(best_models_by_count)\n",
    "\n",
    "            # Save Best Model name to final list of best_models\n",
    "            if len(best_models_by_count) == 1:\n",
    "                best_models.append(best_models_by_count[0])\n",
    "            elif len(best_models_by_count) > 1 & len(best_models_by_count)< len(model_counts):\n",
    "                best_models.extend(best_models_by_count)\n",
    "            else:\n",
    "                best_models.append('Null')  # Handle the case where no significant models are found\n",
    "\n",
    "            print(f\"Successfully read {file_path}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_path} not found.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"File {file_path} is empty.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Determine which model has the highest count of best-performing metrics\n",
    "models_count=pd.Series(best_models).value_counts()\n",
    "\n",
    "# Convert list to Series and count occurrences of each model\n",
    "models_count = pd.Series(best_models).value_counts()\n",
    "\n",
    "# Find the maximum count\n",
    "max_count = models_count.max()\n",
    "\n",
    "# Filter models_count to get model names with the highest count\n",
    "models_with_max_count = models_count[models_count == max_count].index.tolist()\n",
    "\n",
    "\n",
    "# Print Best Model\n",
    "\n",
    "if len(models_with_max_count)==1:\n",
    "    print(\"The Best model based on analysis:\",best_models[0])\n",
    "else:\n",
    "    if len(models_with_max_count) == 1:\n",
    "        print(f\"\\nBest model based on highest count: {best_models_by_count[0]}\")\n",
    "    elif len(models_with_max_count)<len(best_models):\n",
    "        print(f\"\\nMultiple models have the highest count: {', '.join(models_with_max_count)}\")\n",
    "    else:\n",
    "        print(\"\\nNo significant model.\")  # Handle the case where no significant models are found\n",
    "\n",
    "# Calculate and print the total execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c7171",
   "metadata": {},
   "source": [
    "### Part Results for better understanding and inclusion in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a592b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'SVM',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Naive Bayes',\n",
       " 'Logistic Regression',\n",
       " 'Naive Bayes',\n",
       " 'Neural Network',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'SVM',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'SVM',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'SVM',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'SVM',\n",
       " 'Logistic Regression',\n",
       " 'SVM',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'SVM',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'SVM',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Naive Bayes',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Naive Bayes',\n",
       " 'Neural Network',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'SVM',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Naive Bayes',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Naive Bayes',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Naive Bayes',\n",
       " 'Neural Network',\n",
       " 'Naive Bayes',\n",
       " 'Naive Bayes',\n",
       " 'Neural Network',\n",
       " 'Naive Bayes',\n",
       " 'Gradient Boosting',\n",
       " 'Naive Bayes',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'SVM',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Logistic Regression',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Logistic Regression',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Gradient Boosting',\n",
       " 'Naive Bayes',\n",
       " 'Gradient Boosting',\n",
       " 'Neural Network',\n",
       " 'Gradient Boosting']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60decc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gradient Boosting      188\n",
       "Logistic Regression     99\n",
       "Neural Network          42\n",
       "Naive Bayes             12\n",
       "SVM                     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which model has the highest count of best-performing metrics\n",
    "models_count # pd.Series(best_models).value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a282229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the maximum count\n",
    "max_count # models_count.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2793d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gradient Boosting']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter models_count to get model names with the highest count\n",
    "models_with_max_count # models_count[models_count == max_count].index.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e2c980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best model based on analysis: Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "# Print Best Model\n",
    "\n",
    "if len(models_with_max_count)==1:\n",
    "    print(\"The Best model based on analysis:\",best_models[0])\n",
    "else:\n",
    "    if len(models_with_max_count) == 1:\n",
    "        print(f\"\\nBest model based on highest count: {best_models_by_count[0]}\")\n",
    "    elif len(models_with_max_count)<len(best_models):\n",
    "        print(f\"\\nMultiple models have the highest count: {', '.join(models_with_max_count)}\")\n",
    "    else:\n",
    "        print(\"\\nNo significant model.\")  # Handle the case where no significant models are found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992f525",
   "metadata": {},
   "source": [
    "\n",
    "#### From the result, we can conclude, Gradient Boosting is the best method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b74b9",
   "metadata": {},
   "source": [
    "<br>-----------------------------------------------------------------------------------------------------------<br>\n",
    "<div style=\"font-size: 15px; font-family: Verdana, Geneva, sans-serif; color: darkgreen;\">  Hence,  \n",
    "    <b>  Gradient Boosting</b> \n",
    "         is the best method for the dataset under use.<br>-----------------------------------------------------------------------------------<br>\n",
    "</div>\n",
    "<div style=\"font-size: 15px; font-family: Verdana, Geneva, sans-serif; color: darkgreen;\">    Now we need to do tunning of the model for \n",
    "</div>\n",
    " <div style=\"font-size: 18px; font-family: 'Courier New', Courier, monospace; color: darkgreen;\"><b>_________optimum performance_________</b>\n",
    "    <br>-----------------------------------------------------------------------------------<br><br> \n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd8dc8",
   "metadata": {},
   "source": [
    "### <Font color='Green'> ------------------------------------------------------------ END of Model Selection -----------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
